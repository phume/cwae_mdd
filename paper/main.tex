\documentclass[journal]{IEEEtran}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{array}
\begin{document}
\title{CWAE-MMD: Conditional Wasserstein Autoencoders\\for Contextual Anomaly Detection in Transaction Monitoring}
\author{\IEEEauthorblockN{Anonymous Author(s)}\IEEEauthorblockA{Anonymous Institution}}
\maketitle

\begin{abstract}
Contextual anomaly detection---identifying samples whose behavioral features deviate from expectations given their context---is critical for anti-money laundering (AML) systems. Conditional Variational Autoencoders (CVAEs) offer a principled approach by learning context-conditioned reconstructions, but suffer from numerical instability when applied to highly skewed tabular data common in financial applications: the KL divergence regularization requires per-sample variance prediction, which can diverge and produce NaN values. We study CWAE-MMD (Conditional Wasserstein Autoencoder with Maximum Mean Discrepancy), which replaces per-sample KL with aggregate posterior matching via MMD. This enables deterministic encoding without variance prediction, eliminating the primary source of numerical instability while allowing the latent space to form distinct context-dependent clusters. For anomaly scoring, we employ a two-stage pipeline: normalized reconstruction residuals are fed to Isolation Forest, providing non-parametric anomaly scores without assuming a specific error distribution. We systematically evaluate nine CWAE-MMD configurations---spanning scoring strategies, reconstruction losses, and kernel settings---against CVAE, Isolation Forest, and Deep Isolation Forest baselines across 15~datasets. On contextual synthetic benchmarks, CWAE-MMD achieves up to 1.000 AUROC, outperforming IF by 19 points. On real-world benchmarks including PaySim fraud data, CWAE-MMD variants outperform all baselines, achieving 0.913 AUROC where standard IF reaches 0.789.
\end{abstract}

\begin{IEEEkeywords}
contextual anomaly detection, Wasserstein autoencoder, MMD, anti-money laundering, isolation forest
\end{IEEEkeywords}

\section{Introduction}

Contextual anomaly detection identifies samples whose behavioral features deviate from what is expected given their context. In transaction monitoring, a~\$500{,}000 wire transfer may be routine for a corporate treasury account but highly suspicious for a retail checking account. The challenge lies in modeling the context-behavior relationship so that deviations can be measured against the right reference distribution~\cite{bolton2001peer, fatf2012recommendations}.

Conditional Variational Autoencoders (CVAEs)~\cite{sohn2015learning, pol2019anomaly} offer a principled approach to this problem by learning a generative model $p_\theta(\mathbf{x}|\mathbf{z}, \mathbf{c})$ conditioned on context. However, CVAEs suffer from significant practical limitations when applied to real-world tabular data. The KL divergence regularization term, which enforces each sample's posterior $q_\phi(\mathbf{z}|\mathbf{x}, \mathbf{c})$ to match the prior $p(\mathbf{z})$, becomes numerically unstable with highly skewed feature distributions common in financial data. The encoder must predict both mean $\boldsymbol{\mu}$ and variance $\boldsymbol{\sigma}^2$ for the latent distribution, and extreme feature values can cause $\log \boldsymbol{\sigma}^2$ to explode, resulting in NaN values during training. Furthermore, the per-sample KL constraint forces the latent space to be a ``fuzzy'' Gaussian everywhere, potentially causing the model to reconstruct anomalies too well by learning an overly smooth representation.

Wasserstein Autoencoders (WAEs)~\cite{tolstikhin2018wasserstein} address these stability issues by replacing the per-sample KL divergence with aggregate posterior matching via Maximum Mean Discrepancy (MMD)~\cite{gretton2012kernel}. Rather than constraining each $q(\mathbf{z}|\mathbf{x})$ individually, WAE enforces that the aggregate posterior $Q_Z = \mathbb{E}_{p(\mathbf{x})}[q(\mathbf{z}|\mathbf{x})]$ matches the prior $P_Z$. This formulation allows deterministic encoders (no variance prediction), eliminates sampling noise, and permits the latent space to form distinct clusters while maintaining global distributional properties. However, the original WAE is unconditional and not directly applicable to contextual anomaly detection.

In this paper, we study CWAE-MMD (Conditional Wasserstein Autoencoder with Maximum Mean Discrepancy), which combines the contextual modeling capability of CVAE with the numerical stability of WAE. The approach conditions both encoder and decoder on context features $\mathbf{c}$, uses MMD to match the aggregate posterior to a Gaussian prior, and employs a deterministic encoder for reproducible inference. For anomaly scoring, we employ a two-stage pipeline: normalized reconstruction residuals are fed to an Isolation Forest~\cite{liu2008isolation}, providing non-parametric anomaly scores without assuming a specific error distribution.

We systematically evaluate multiple CWAE-MMD configurations and make the following contributions:
\begin{itemize}
\item We combine conditional autoencoding with aggregate posterior matching via MMD, inheriting the stability of WAE while enabling contextual anomaly detection.
\item We study a two-stage anomaly scoring pipeline where Isolation Forest operates on normalized reconstruction residuals, decoupling the reconstruction model from the scoring mechanism.
\item We evaluate nine CWAE-MMD configurations---spanning scoring strategies (IF, DIF, latent, dual-space), reconstruction losses (MSE, CRPS, Huber), and kernel settings (standard IMQ, IMQ-Extreme)---identifying when each variant is most effective.
\item We provide experimental evaluation across 15~datasets comparing CWAE-MMD against CVAE, Isolation Forest, and Deep Isolation Forest baselines.
\end{itemize}

The remainder of this paper is organized as follows. Section~II presents the CWAE-MMD method, including the base architecture, all scoring and loss variants, and the relationship to prior work. Section~III describes experimental results. Section~IV discusses findings and concludes.

%====================================================================
\section{CWAE-MMD Method}
%====================================================================

\subsection{Background: From CVAE to WAE}

CVAEs~\cite{sohn2015learning, kingma2014auto} learn a stochastic encoder $q_\phi(\mathbf{z}|\mathbf{x}, \mathbf{c})$ and decoder $p_\theta(\mathbf{x}|\mathbf{z}, \mathbf{c})$, optimizing the evidence lower bound (ELBO):
\begin{equation}
\mathcal{L}_{\mathrm{CVAE}} = \mathbb{E}_{q_\phi}[\log p_\theta(\mathbf{x}|\mathbf{z}, \mathbf{c})] - \beta \cdot \mathrm{KL}[q_\phi(\mathbf{z}|\mathbf{x}, \mathbf{c}) \| p(\mathbf{z})]
\end{equation}
The KL term requires per-sample distribution parameters $\boldsymbol{\mu}(\mathbf{x}, \mathbf{c})$ and $\log\boldsymbol{\sigma}^2(\mathbf{x}, \mathbf{c})$:
\begin{equation}
\mathrm{KL} = -\tfrac{1}{2}\textstyle\sum_j\bigl(1 + \log\sigma_j^2 - \mu_j^2 - \sigma_j^2\bigr)
\end{equation}
This formulation has three instability sources: (1)~$\log\sigma^2 \to -\infty$ when the encoder predicts near-zero variance; (2)~$\log\sigma^2 \to +\infty$ causes $\exp(\log\sigma^2)$ to overflow; (3)~the reparameterization trick $\mathbf{z} = \boldsymbol{\mu} + \boldsymbol{\sigma} \odot \boldsymbol{\epsilon}$ amplifies noise when $\boldsymbol{\sigma}$ is extreme.

Wasserstein Autoencoders~\cite{tolstikhin2018wasserstein} replace per-sample KL with aggregate posterior matching. The WAE objective is:
\begin{equation}
\mathcal{L}_{\mathrm{WAE}} = \mathbb{E}_{p(\mathbf{x})}[c(\mathbf{x}, G(E(\mathbf{x})))] + \lambda \cdot D_Z(Q_Z, P_Z)
\end{equation}
where $E$ is the encoder, $G$ is the decoder, $c(\cdot,\cdot)$ is a reconstruction cost, and $D_Z$ measures divergence between the aggregate posterior and prior. The WAE-MMD variant uses Maximum Mean Discrepancy:
\begin{equation}
\mathrm{MMD}^2(P, Q) = \mathbb{E}[k(\mathbf{z}, \mathbf{z}')] - 2\mathbb{E}[k(\mathbf{z}, \tilde{\mathbf{z}})] + \mathbb{E}[k(\tilde{\mathbf{z}}, \tilde{\mathbf{z}}')]
\end{equation}
where $\mathbf{z}, \mathbf{z}' \sim P_Z$, $\tilde{\mathbf{z}}, \tilde{\mathbf{z}}' \sim Q_Z$, and $k(\cdot,\cdot)$ is a kernel function. The inverse multiquadrics (IMQ) kernel $k(\mathbf{x}, \mathbf{y}) = C/(C + \|\mathbf{x} - \mathbf{y}\|^2)$ is preferred for its heavier tails compared to RBF, preventing vanishing gradients for distant points.

WAE offers key advantages: (1)~deterministic encoders are permitted since no reparameterization trick is needed, (2)~no variance prediction eliminates $\log/\exp$ numerical instability, and (3)~the aggregate constraint allows latent codes to form distinct clusters while maintaining global Gaussian statistics. However, the original WAE is unconditional.

\subsection{CWAE-MMD Architecture}

CWAE-MMD extends WAE to the conditional setting. The encoder maps behavioral features and context to a latent code:
\begin{equation}
\mathbf{z} = E_\phi(\mathbf{x}, \mathbf{c}) = \mathrm{MLP}_\phi([\mathbf{x}; \mathbf{c}])
\end{equation}
where $[\mathbf{x}; \mathbf{c}]$ denotes concatenation and $\mathbf{z} \in \mathbb{R}^{d_z}$. Critically, the encoder is deterministic---it outputs a point estimate rather than distribution parameters. The decoder reconstructs behavioral features:
\begin{equation}
\hat{\mathbf{x}} = D_\theta(\mathbf{z}, \mathbf{c}) = \mathrm{MLP}_\theta([\mathbf{z}; \mathbf{c}])
\end{equation}
Both encoder and decoder are multi-layer perceptrons with hidden layers $[128, 64]$ and ReLU activations. Context $\mathbf{c}$ conditions both networks, allowing the model to learn context-dependent reconstructions.

The training objective combines reconstruction loss with MMD regularization:
\begin{equation}
\mathcal{L} = \mathcal{L}_{\mathrm{recon}}(\mathbf{x}, \hat{\mathbf{x}}) + \lambda \cdot \mathrm{MMD}^2(Q_Z, P_Z)
\label{eq:loss}
\end{equation}
We use multi-scale IMQ kernels with bandwidth $C_s = s \cdot 2d_z$ at scales $s \in \{0.2, 0.5, 1.0, 2.0, 5.0\}$. In practice, MMD is estimated on mini-batches of size $B$:
\begin{equation}
\widehat{\mathrm{MMD}}^2 = \frac{1}{B(B{-}1)}\!\sum_{i \neq j}\!k(\mathbf{z}_i, \mathbf{z}_j) - \frac{2}{B^2}\!\sum_{i,j}\!k(\mathbf{z}_i, \tilde{\mathbf{z}}_j) + \frac{1}{B(B{-}1)}\!\sum_{i \neq j}\!k(\tilde{\mathbf{z}}_i, \tilde{\mathbf{z}}_j)
\end{equation}

Unlike KL divergence where $\log\sigma^2$ can diverge, the IMQ kernel is bounded in $(0, 1]$, ensuring $\mathrm{MMD}^2 \in [0, 2]$ with bounded gradients. This eliminates the need for log-variance clamping commonly required in CVAE implementations. We optimize using Adam with learning rate $10^{-3}$ and weight decay $10^{-5}$, with early stopping on validation loss.

\subsection{Scoring Variants}

After training, anomaly scores are derived from reconstruction residuals $\mathbf{r} = \mathbf{x} - \hat{\mathbf{x}}$, normalized by training residual standard deviation: $\mathbf{r}_{\mathrm{norm}} = \mathbf{r} / \boldsymbol{\sigma}_{\mathrm{train}}$.

\textbf{CWAE-IF} (base scorer). Fits an Isolation Forest~\cite{liu2008isolation} on $\mathbf{r}_{\mathrm{norm}}$: $s_i = -\mathrm{IF}(\mathbf{r}_{\mathrm{norm},i})$.
\textit{Pro:} Simple, fast, well-understood.
\textit{Con:} Axis-aligned splits may miss non-linear structure in residual space.

\textbf{CWAE-DIF} (Deep Isolation Forest~\cite{xu2023deep}). Applies $M{=}6$ frozen random MLP projections $\phi_m$ before IF:
\begin{equation}
s_i = \frac{1}{M}\sum_{m=1}^{M} -\mathrm{IF}_m\bigl(\phi_m(\mathbf{r}_{\mathrm{norm},i})\bigr)
\end{equation}
\textit{Pro:} Non-linear anomaly detection via random projections.
\textit{Con:} $M$-fold slower than IF.

\textbf{CWAE-IF-Latent}. Fits IF on latent codes instead of residuals: $s_i = -\mathrm{IF}(\mathbf{z}_i)$.
\textit{Pro:} Detects anomalies in the learned representation; complementary to residual scoring.
\textit{Con:} Low-dimensional latent space ($d_z{=}32$) may lack discriminative power for some datasets.

\textbf{CWAE-IF-Dual}. Ensemble of residual and latent scores with max aggregation:
\begin{equation}
s_i = \max\!\bigl(\tilde{s}^{\mathrm{res}}_i,\; \tilde{s}^{\mathrm{lat}}_i\bigr)
\end{equation}
where $\tilde{s}$ denotes min-max normalized scores from independent IF models.
\textit{Pro:} Catches anomalies detectable in either representation.
\textit{Con:} Requires fitting two IF models.

\textbf{CWAE-DIF-Dual}. Combines DIF with dual-space aggregation---DIF on both residuals and latent codes, then max aggregation.
\textit{Pro:} Most expressive variant (non-linear projections + dual space).
\textit{Con:} Heaviest computation ($2M$ projections + $2M$ IF calls).

\subsection{Loss Variants}

\textbf{MSE} (default). Mean squared reconstruction error:
$\mathcal{L}_{\mathrm{recon}} = \frac{1}{B}\sum_{i}\|\mathbf{x}_i - \hat{\mathbf{x}}_i\|^2$.
\textit{Pro:} Simple, smooth gradients everywhere.
\textit{Con:} Quadratic penalty amplifies the influence of outliers, which are common in financial data.

\textbf{CRPS}~\cite{gneiting2007strictly}. Continuous Ranked Probability Score under a Gaussian assumption:
\begin{equation}
\mathcal{L}_{\mathrm{CRPS}} = \hat{\sigma}\!\left[z\bigl(2\Phi(z)-1\bigr) + 2\phi(z) - \frac{1}{\sqrt{\pi}}\right]
\end{equation}
where $z = (\mathbf{x} - \hat{\mathbf{x}})/\hat{\sigma}$, $\hat{\sigma}$ is the batch residual standard deviation, and $\Phi$, $\phi$ are the standard normal CDF and PDF.
\textit{Pro:} Robust to outliers; proper scoring rule with probabilistic interpretation.
\textit{Con:} Requires estimating $\hat{\sigma}$ per batch; slightly slower.

\textbf{Huber}~\cite{huber1964robust}. Smooth blend of $L_2$ near zero and $L_1$ in the tails:
\begin{equation}
\mathcal{L}_{\mathrm{Huber}}(r, \delta) = \begin{cases} \frac{1}{2}r^2 & \text{if } |r| \le \delta \\ \delta\bigl(|r| - \frac{\delta}{2}\bigr) & \text{otherwise}\end{cases}
\end{equation}
\textit{Pro:} Smooth gradient at origin (good for optimization) while being robust to large residuals.
\textit{Con:} Threshold $\delta$ is a hyperparameter that requires tuning.

\subsection{Kernel Variants}

\textbf{Standard IMQ}. Bandwidth scales $\{0.2, 0.5, 1.0, 2.0, 5.0\}$. Sufficient for well-behaved latent distributions where most mass is concentrated near the origin.

\textbf{IMQ-Extreme}. Extended scales $\{0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0\}$.
\textit{Pro:} Captures similarities at extreme distances, better for heavy-tailed latent spaces.
\textit{Con:} More kernel evaluations per batch.

\subsection{Preprocessing Variants}

\textbf{None} (default). Standard scaling only.

\textbf{Tail Compression}. A monotonic, invertible transform that compresses values above a quantile threshold $\tau$:
\begin{equation}
\tilde{x} = \begin{cases} x & \text{if } x \le \tau \\ \log(x - \tau + 1) + \tau & \text{if } x > \tau \end{cases}
\end{equation}
\textit{Pro:} Reduces reconstruction loss variance on heavy-tailed financial data (e.g., transaction amounts spanning \$0--\$10M).
\textit{Con:} May compress informative tail structure if anomalies manifest as extreme values.

\subsection{Baselines}

\textbf{CVAE}~\cite{sohn2015learning}: Stochastic encoder producing $\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\sigma}^2)$ with KL regularization. Requires learning per-sample variance, which can diverge on skewed data.
\textbf{IF}~\cite{liu2008isolation}: Isolation Forest on behavioral features only (ignores context entirely).
\textbf{IF-concat}: IF on concatenated $[\mathbf{x}; \mathbf{c}]$ (treats context as additional features rather than a conditioning variable).
\textbf{DIF}~\cite{xu2023deep}: Deep Isolation Forest with frozen random MLP projections on $[\mathbf{x}; \mathbf{c}]$.

%====================================================================
\section{Experiments}
%====================================================================

We evaluate 16 methods across 4~synthetic and 11~real datasets, reporting mean AUROC over 3~seeds $\{42, 123, 456\}$.

\begin{table*}[t]
\centering
\caption{AUROC on Synthetic Contextual Datasets (mean over 3 seeds). \textbf{Bold}~=~best per column.}
\label{tab:synthetic}
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{Linear} & \textbf{Scale} & \textbf{Nonlinear} & \textbf{Multimodal} \\
\midrule
IF & .833 & .636 & .832 & \textbf{.998} \\
IF-concat & .833 & .636 & .832 & \textbf{.998} \\
DIF & .977 & \textbf{.685} & .987 & .838 \\
CVAE & .953 & .673 & .989 & .591 \\
\midrule
CWAE-IF & .988 & .634 & .997 & .641 \\
CWAE-DIF & .988 & .632 & .997 & .656 \\
CWAE-IF-Latent & .994 & .675 & .996 & .587 \\
CWAE-IF-Dual & \textbf{.995} & .662 & \textbf{1.000} & .585 \\
CWAE-DIF-Dual & .994 & .664 & \textbf{1.000} & .594 \\
CWAE-CRPS & .990 & .644 & .999 & .661 \\
CWAE-CRPS-IF & .990 & .644 & .999 & .661 \\
CWAE-Huber & .985 & .664 & .999 & .731 \\
CWAE-IMQ-Ext & .981 & .649 & .998 & .652 \\
CWAE-Beta0.1 & .981 & .636 & .995 & .723 \\
CWAE-Beta10 & .980 & .652 & .993 & .612 \\
CWAE-Latent16 & .985 & .659 & .996 & .613 \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Synthetic results} (Table~\ref{tab:synthetic}). On \textit{Linear} and \textit{Nonlinear} benchmarks, where anomalies are contextual (normal globally, unusual for their context), CWAE-MMD variants achieve 0.988--1.000 AUROC, outperforming IF~(0.833) by up to 19~points. CWAE-IF-Dual and CWAE-DIF-Dual reach perfect 1.000 on Nonlinear, demonstrating that dual-space scoring captures complex anomaly structure. On \textit{Scale} (context-dependent variance), all methods struggle; DIF leads at 0.685. On \textit{Multimodal} (categorical context with swap anomalies), IF achieves near-perfect 0.998---these are effectively global anomalies detectable without context modeling, confirming that contextual methods provide no advantage when anomalies are not context-dependent.

\begin{table*}[t]
\centering
\caption{AUROC on Real-World Datasets (mean over 3 seeds). \textbf{Bold}~=~best per column.}
\label{tab:real}
\small
\setlength{\tabcolsep}{3.5pt}
\begin{tabular}{lccccccccccc}
\toprule
\textbf{Method} & \rotatebox{70}{\textbf{Thyroid}} & \rotatebox{70}{\textbf{Cardio}} & \rotatebox{70}{\textbf{Arrhyth.}} & \rotatebox{70}{\textbf{Ionosph.}} & \rotatebox{70}{\textbf{Pima}} & \rotatebox{70}{\textbf{WBC}} & \rotatebox{70}{\textbf{Vowels}} & \rotatebox{70}{\textbf{SAML-D}} & \rotatebox{70}{\textbf{PaySim}} & \rotatebox{70}{\textbf{CreditCd}} & \rotatebox{70}{\textbf{IEEE-CIS}} \\
\midrule
IF           & .962 & .855 & .784 & .852 & .651 & .966 & .662 & .514 & .789 & .949 & .606 \\
IF-concat    & \textbf{.990} & .954 & \textbf{.794} & .908 & \textbf{.723} & .958 & .753 & \textbf{.703} & .842 & .949 & \textbf{.642} \\
DIF          & .964 & \textbf{.964} & .759 & \textbf{.940} & .708 & .959 & .843 & .683 & .778 & .947 & .586 \\
CVAE         & .879 & .725 & .779 & .919 & .662 & .964 & .838 & .618 & .873 & .952 & .561 \\
\midrule
CWAE-IF      & .890 & .842 & .787 & .934 & .672 & .974 & .906 & .666 & \textbf{.913} & .957 & .592 \\
CWAE-CRPS    & .928 & .816 & .782 & .931 & .698 & \textbf{.978} & \textbf{.925} & .629 & .898 & .956 & .639 \\
CWAE-Huber   & .886 & .814 & .786 & .934 & .664 & .970 & .899 & .640 & .911 & .959 & .613 \\
CWAE-IF-Lat  & .964 & .872 & .723 & .616 & .617 & .911 & .702 & .639 & .778 & .952 & .552 \\
CWAE-IF-Dual & .956 & .879 & .787 & .823 & .675 & .966 & .861 & .672 & .902 & .959 & .593 \\
CWAE-DIF-D   & .954 & .885 & .771 & .882 & .680 & .959 & .882 & .672 & .910 & \textbf{.959} & .603 \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{Real-world results} (Table~\ref{tab:real}). No single method dominates across all 11~datasets. IF-concat wins on 5~datasets (Thyroid, Arrhythmia, Pima, SAML-D, IEEE-CIS), DIF on 2~(Cardio, Ionosphere), and CWAE-MMD variants on 4~(WBC, Vowels, PaySim, CreditCard).

CWAE-CRPS emerges as the strongest CWAE variant, achieving best-in-class on WBC~(0.978) and Vowels~(0.925). On \textit{PaySim}---a financial fraud dataset with context-dependent transaction patterns---CWAE variants (0.898--0.913) substantially outperform all baselines including IF-concat~(0.842) and IF~(0.789), providing the strongest evidence for contextual modeling in fraud detection. On CreditCard, several CWAE variants (0.957--0.959) edge out baselines. Conversely, on datasets where anomalies are global rather than contextual (Thyroid, SAML-D), IF-concat suffices and CWAE-MMD provides no advantage.

Notably, plain IF (ignoring context) is consistently the weakest method, ranking last or near-last on 8 of 11~real datasets. This confirms that incorporating context features---whether by concatenation or conditional modeling---is important for real-world anomaly detection.

%====================================================================
\section{Discussion and Conclusion}
%====================================================================

CWAE-MMD provides the largest advantage when anomalies are contextual: 0.99+ AUROC on synthetic benchmarks where IF achieves 0.83. On real datasets with context-behavior structure (PaySim, Vowels, WBC), CWAE-MMD consistently outperforms baselines. When anomalies are global (Thyroid, SAML-D), simpler methods like IF-concat suffice.

\textbf{Variant guidance.} Based on our evaluation: (1)~CWAE-CRPS is the recommended default---its robust loss handles heavy-tailed data while maintaining competitive performance across settings; (2)~CWAE-DIF-Dual for high-dimensional behavioral features where non-linear residual structure is expected; (3)~IF-concat when anomalies are suspected to be global rather than contextual. Across all 15~datasets, CWAE-MMD produced zero NaN training runs, confirming its numerical stability advantage over CVAE.

\textbf{Limitations.} CWAE-MMD requires a training phase, unlike tree-based methods that operate in a single pass. The two-stage pipeline (train autoencoder, then fit IF) introduces additional hyperparameters. On datasets where context does not explain behavioral variation (e.g., SAML-D), the conditional model provides no benefit over simpler approaches.

An extended version with AUPRC analysis, scalability studies, and ablation experiments is in preparation.

\bibliographystyle{IEEEtran}
\bibliography{references}
\end{document}
